{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "euGB9Mmqx3Pd",
        "outputId": "8ff8f1ec-8757-40a5-ab84-fc1f252ee99b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   Time        V1        V2        V3        V4        V5        V6        V7  \\\n",
            "0     0 -1.359807 -0.072781  2.536347  1.378155 -0.338321  0.462388  0.239599   \n",
            "1     0  1.191857  0.266151  0.166480  0.448154  0.060018 -0.082361 -0.078803   \n",
            "2     1 -1.358354 -1.340163  1.773209  0.379780 -0.503198  1.800499  0.791461   \n",
            "3     1 -0.966272 -0.185226  1.792993 -0.863291 -0.010309  1.247203  0.237609   \n",
            "4     2 -1.158233  0.877737  1.548718  0.403034 -0.407193  0.095921  0.592941   \n",
            "\n",
            "         V8        V9  ...       V21       V22       V23       V24       V25  \\\n",
            "0  0.098698  0.363787  ... -0.018307  0.277838 -0.110474  0.066928  0.128539   \n",
            "1  0.085102 -0.255425  ... -0.225775 -0.638672  0.101288 -0.339846  0.167170   \n",
            "2  0.247676 -1.514654  ...  0.247998  0.771679  0.909412 -0.689281 -0.327642   \n",
            "3  0.377436 -1.387024  ... -0.108300  0.005274 -0.190321 -1.175575  0.647376   \n",
            "4 -0.270533  0.817739  ... -0.009431  0.798278 -0.137458  0.141267 -0.206010   \n",
            "\n",
            "        V26       V27       V28  Amount  Class  \n",
            "0 -0.189115  0.133558 -0.021053  149.62      0  \n",
            "1  0.125895 -0.008983  0.014724    2.69      1  \n",
            "2 -0.139097 -0.055353 -0.059752  378.66      0  \n",
            "3 -0.221929  0.062723  0.061458  123.50      0  \n",
            "4  0.502292  0.219422  0.215153   69.99      0  \n",
            "\n",
            "[5 rows x 31 columns]\n",
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 772 entries, 0 to 771\n",
            "Data columns (total 31 columns):\n",
            " #   Column  Non-Null Count  Dtype  \n",
            "---  ------  --------------  -----  \n",
            " 0   Time    772 non-null    int64  \n",
            " 1   V1      772 non-null    float64\n",
            " 2   V2      772 non-null    float64\n",
            " 3   V3      772 non-null    float64\n",
            " 4   V4      772 non-null    float64\n",
            " 5   V5      772 non-null    float64\n",
            " 6   V6      772 non-null    float64\n",
            " 7   V7      772 non-null    float64\n",
            " 8   V8      772 non-null    float64\n",
            " 9   V9      772 non-null    float64\n",
            " 10  V10     772 non-null    float64\n",
            " 11  V11     772 non-null    float64\n",
            " 12  V12     772 non-null    float64\n",
            " 13  V13     772 non-null    float64\n",
            " 14  V14     772 non-null    float64\n",
            " 15  V15     772 non-null    float64\n",
            " 16  V16     772 non-null    float64\n",
            " 17  V17     772 non-null    float64\n",
            " 18  V18     772 non-null    float64\n",
            " 19  V19     772 non-null    float64\n",
            " 20  V20     772 non-null    float64\n",
            " 21  V21     772 non-null    float64\n",
            " 22  V22     772 non-null    float64\n",
            " 23  V23     772 non-null    float64\n",
            " 24  V24     772 non-null    float64\n",
            " 25  V25     772 non-null    float64\n",
            " 26  V26     772 non-null    float64\n",
            " 27  V27     772 non-null    float64\n",
            " 28  V28     772 non-null    float64\n",
            " 29  Amount  772 non-null    float64\n",
            " 30  Class   772 non-null    int64  \n",
            "dtypes: float64(29), int64(2)\n",
            "memory usage: 187.1 KB\n",
            "None\n",
            "Original class distribution:\n",
            "Class\n",
            "0    763\n",
            "1      9\n",
            "Name: count, dtype: int64\n",
            "Balanced class distribution:\n",
            "Class\n",
            "0    763\n",
            "1    763\n",
            "Name: count, dtype: int64\n",
            "Processing sampling technique: Technique1...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/base.py:474: FutureWarning: `BaseEstimator._validate_data` is deprecated in 1.6 and will be removed in 1.7. Use `sklearn.utils.validation.validate_data` instead. This function becomes public and is part of the scikit-learn developer API.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/utils/_tags.py:354: FutureWarning: The SMOTE or classes from which it inherits use `_get_tags` and `_more_tags`. Please define the `__sklearn_tags__` method, or inherit from `sklearn.base.BaseEstimator` and/or other appropriate mixins such as `sklearn.base.TransformerMixin`, `sklearn.base.ClassifierMixin`, `sklearn.base.RegressorMixin`, and `sklearn.base.OutlierMixin`. From scikit-learn 1.7, not defining `__sklearn_tags__` will raise an error.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/xgboost/core.py:158: UserWarning: [15:09:49] WARNING: /workspace/src/learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LightGBM] [Info] Number of positive: 354, number of negative: 346\n",
            "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000443 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 7011\n",
            "[LightGBM] [Info] Number of data points in the train set: 700, number of used features: 30\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.505714 -> initscore=0.022858\n",
            "[LightGBM] [Info] Start training from score 0.022858\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "Processing sampling technique: Technique2...\n",
            "[LightGBM] [Info] Number of positive: 358, number of negative: 342\n",
            "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001231 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 6997\n",
            "[LightGBM] [Info] Number of data points in the train set: 700, number of used features: 30\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.511429 -> initscore=0.045722\n",
            "[LightGBM] [Info] Start training from score 0.045722\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/xgboost/core.py:158: UserWarning: [15:09:52] WARNING: /workspace/src/learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "Processing sampling technique: Technique3...\n",
            "[LightGBM] [Info] Number of positive: 550, number of negative: 518\n",
            "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000668 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 7638\n",
            "[LightGBM] [Info] Number of data points in the train set: 1068, number of used features: 30\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.514981 -> initscore=0.059943\n",
            "[LightGBM] [Info] Start training from score 0.059943\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/xgboost/core.py:158: UserWarning: [15:09:53] WARNING: /workspace/src/learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing sampling technique: Technique4...\n",
            "[LightGBM] [Info] Number of positive: 337, number of negative: 363\n",
            "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000486 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 7011\n",
            "[LightGBM] [Info] Number of data points in the train set: 700, number of used features: 30\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.481429 -> initscore=-0.074320\n",
            "[LightGBM] [Info] Start training from score -0.074320\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/xgboost/core.py:158: UserWarning: [15:09:55] WARNING: /workspace/src/learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "Processing sampling technique: Technique5...\n",
            "[LightGBM] [Info] Number of positive: 338, number of negative: 362\n",
            "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000484 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 7004\n",
            "[LightGBM] [Info] Number of data points in the train set: 700, number of used features: 30\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.482857 -> initscore=-0.068598\n",
            "[LightGBM] [Info] Start training from score -0.068598\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/xgboost/core.py:158: UserWarning: [15:09:57] WARNING: /workspace/src/learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "Accuracy Matrix:\n",
            "            NaiveBayes   XGBoost  LightGBM  Perceptron  GradBoost\n",
            "Technique1    0.773333  0.980000  0.990000    0.716667   0.986667\n",
            "Technique2    0.876667  0.970000  0.983333    0.503333   0.973333\n",
            "Technique3    0.838428  0.980349  0.986900    0.462882   0.982533\n",
            "Technique4    0.866667  0.986667  0.993333    0.600000   0.990000\n",
            "Technique5    0.783333  0.990000  0.986667    0.620000   0.980000\n",
            "Best Sampling Technique for Each Model:\n",
            "NaiveBayes    Technique2\n",
            "XGBoost       Technique5\n",
            "LightGBM      Technique4\n",
            "Perceptron    Technique1\n",
            "GradBoost     Technique4\n",
            "dtype: object\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from xgboost import XGBClassifier\n",
        "from lightgbm import LGBMClassifier\n",
        "from sklearn.linear_model import Perceptron\n",
        "from sklearn.ensemble import GradientBoostingClassifier\n",
        "from imblearn.over_sampling import SMOTE\n",
        "\n",
        "# Load the dataset\n",
        "data = pd.read_csv('/content/Creditcard_data.csv')\n",
        "print(data.head())\n",
        "print(data.info())\n",
        "\n",
        "# Separate features and target variable\n",
        "X = data.drop(\"Class\", axis=1)\n",
        "y = data[\"Class\"]\n",
        "\n",
        "# Apply SMOTE to balance the dataset\n",
        "smote = SMOTE(random_state=10)\n",
        "X_balanced, y_balanced = smote.fit_resample(X, y)\n",
        "\n",
        "# Display class distributions\n",
        "print(\"Original class distribution:\")\n",
        "print(y.value_counts())\n",
        "print(\"Balanced class distribution:\")\n",
        "print(y_balanced.value_counts())\n",
        "\n",
        "# Define sample size\n",
        "sample_size = 1000\n",
        "\n",
        "# Generate different samples\n",
        "sampled_data = {\n",
        "    \"Technique1\": X_balanced.sample(n=sample_size, random_state=15),\n",
        "    \"Technique2\": X_balanced.sample(n=sample_size, random_state=25),\n",
        "    \"Technique3\": X_balanced.iloc[::len(X_balanced) // sample_size, :],\n",
        "    \"Technique4\": X_balanced.sample(n=sample_size, random_state=35),\n",
        "    \"Technique5\": X_balanced.sample(n=sample_size, random_state=45),\n",
        "}\n",
        "\n",
        "# Link samples with their target labels\n",
        "sample_targets = {\n",
        "    name: (sample, y_balanced.loc[sample.index])\n",
        "    for name, sample in sampled_data.items()\n",
        "}\n",
        "\n",
        "# Define alternative models\n",
        "ml_models = {\n",
        "    \"NaiveBayes\": GaussianNB(),\n",
        "    \"XGBoost\": XGBClassifier(use_label_encoder=False, eval_metric='logloss', random_state=42),\n",
        "    \"LightGBM\": LGBMClassifier(random_state=42),\n",
        "    \"Perceptron\": Perceptron(random_state=42),\n",
        "    \"GradBoost\": GradientBoostingClassifier(random_state=42),\n",
        "}\n",
        "\n",
        "# Store results\n",
        "accuracy_results = {}\n",
        "\n",
        "# Train and evaluate models\n",
        "for technique_name, (X_sample, y_sample) in sample_targets.items():\n",
        "    print(f\"Processing sampling technique: {technique_name}...\")\n",
        "    X_train, X_test, y_train, y_test = train_test_split(X_sample, y_sample, test_size=0.3, random_state=20)\n",
        "\n",
        "    for model_name, model in ml_models.items():\n",
        "        model.fit(X_train, y_train)\n",
        "        predictions = model.predict(X_test)\n",
        "        acc = accuracy_score(y_test, predictions)\n",
        "\n",
        "        if technique_name not in accuracy_results:\n",
        "            accuracy_results[technique_name] = {}\n",
        "        accuracy_results[technique_name][model_name] = acc\n",
        "\n",
        "# Convert results to DataFrame\n",
        "accuracy_matrix = pd.DataFrame.from_dict(accuracy_results, orient='index')\n",
        "\n",
        "print(\"Accuracy Matrix:\")\n",
        "print(accuracy_matrix)\n",
        "\n",
        "# Save accuracy matrix to CSV\n",
        "accuracy_matrix.to_csv(\"accuracy_matrix.csv\")\n",
        "\n",
        "# Find the best sampling technique for each model\n",
        "best_methods = accuracy_matrix.idxmax()\n",
        "print(\"Best Sampling Technique for Each Model:\")\n",
        "print(best_methods)\n",
        "\n",
        "# Save best combinations to a CSV file\n",
        "best_methods.to_csv(\"best_combinations.csv\")\n"
      ]
    }
  ]
}